{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6267b8f8",
   "metadata": {},
   "source": [
    "好的，我来把这段作业说明完整翻译成中文，保持原有的层次和要求。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 训练 BPE 分词器实验\n",
    "\n",
    "我们将要在 **TinyStories 数据集** 上训练一个 **字节级 BPE（Byte Pair Encoding）分词器**。\n",
    "（关于如何查找 / 下载该数据集的说明，请见 **第 1 节**。）\n",
    "在开始前，我们建议先浏览一下 TinyStories 数据集，以便对数据内容有一个直观的认识。\n",
    "\n",
    "---\n",
    "\n",
    "#### 并行化预分词（Pre-tokenization）\n",
    "\n",
    "你会发现，最大的瓶颈在于 **预分词步骤**。\n",
    "你可以通过 **多进程库 multiprocessing** 来并行化预分词，从而加速。\n",
    "\n",
    "具体来说，建议的做法是：\n",
    "\n",
    "* 将语料划分为多个 **chunk（块）**。\n",
    "* 确保 **chunk 边界出现在特殊标记的开头**。\n",
    "\n",
    "👉 我们推荐直接使用下面提供的 starter code 来获取 chunk 边界：\n",
    "[https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336\\_basics/pretokenization\\_example.py](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py)\n",
    "\n",
    "这样切分总是合法的，因为我们永远不希望跨文档边界进行 merge 操作。\n",
    "在作业中，你可以始终采用这种方式，不必担心「超大语料中没有 `<|endoftext|>`」的边缘情况。\n",
    "\n",
    "---\n",
    "\n",
    "#### 在预分词前移除特殊标记\n",
    "\n",
    "在运行正则表达式（`re.finditer`）进行预分词之前，\n",
    "你应该先从语料（或每个 chunk）中去除所有 **特殊标记**。\n",
    "\n",
    "⚠️ 关键点：要对语料进行 **基于特殊标记的切分**，保证 **不会在特殊标记两侧合并**。\n",
    "\n",
    "例如：\n",
    "\n",
    "```\n",
    "[Doc 1]<|endoftext|>[Doc 2]\n",
    "```\n",
    "\n",
    "应当被切分为：\n",
    "\n",
    "* 对 `[Doc 1]` 单独预分词\n",
    "* 对 `[Doc 2]` 单独预分词\n",
    "\n",
    "这样就不会跨文档边界合并。\n",
    "\n",
    "实现方法：\n",
    "可以使用 `re.split`，以 `\"|\".join(special_tokens)` 作为分隔符（注意要配合 `re.escape`，因为特殊标记中可能包含 `|`）。\n",
    "\n",
    "👉 测试用例 **test\\_train\\_bpe\\_special\\_tokens** 会检查这一点。\n",
    "\n",
    "---\n",
    "\n",
    "#### 优化合并步骤（BPE Training）\n",
    "\n",
    "朴素的 BPE 训练实现非常慢，因为：\n",
    "\n",
    "* 每次合并都要重新扫描所有字节对，找到出现频率最高的 pair。\n",
    "\n",
    "改进思路：\n",
    "\n",
    "* 实际上，每次合并后，只有 **与被合并 pair 相邻的 pair 频率** 会发生变化。\n",
    "* 因此，可以通过 **维护 pair 频率的索引（缓存）并增量更新** 来加速，而不是每次都全量扫描。\n",
    "\n",
    "⚠️ 注意：BPE 训练的 **合并部分无法在 Python 中并行化**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 低资源 / 下采样（Downscaling）提示\n",
    "\n",
    "* 使用 **cProfile** 或 **scalene** 等分析工具，定位瓶颈代码，并集中优化。\n",
    "* 在正式跑大规模训练前，**先用小数据子集调试**。\n",
    "\n",
    "  * 例如，可以先在 TinyStories **验证集（22K 文档）** 上训练，而不是完整的 **2.12M 文档**。\n",
    "\n",
    "这样做的意义：\n",
    "\n",
    "* 调试时更快。\n",
    "* 保证小数据集仍然能复现大数据集的性能瓶颈，从而优化能迁移到正式环境。\n",
    "\n",
    "---\n",
    "\n",
    "### 题目 (train\\_bpe)：BPE 分词器训练（15 分）\n",
    "\n",
    "**任务**：编写一个函数，给定输入文本路径，训练一个 **字节级 BPE 分词器**。\n",
    "\n",
    "函数需支持以下参数：\n",
    "\n",
    "* `input_path: str` —— 训练数据文件路径。\n",
    "* `vocab_size: int` —— 最大词表大小（包括初始字节词表、合并得到的新词，以及所有特殊标记）。\n",
    "* `special_tokens: list[str]` —— 需要添加到词表中的特殊标记。⚠️ 这些特殊标记**不影响 BPE 训练过程**。\n",
    "\n",
    "函数返回：\n",
    "\n",
    "* `vocab: dict[int, bytes]` —— 分词器词表（token ID → 对应字节序列）。\n",
    "* `merges: list[tuple[bytes, bytes]]` —— 训练得到的 BPE 合并操作序列，每一项是一个合并的 token 对，顺序为生成顺序。\n",
    "\n",
    "测试方法：\n",
    "\n",
    "* 先实现 **\\[adapters.run\\_train\\_bpe]** 测试适配器。\n",
    "* 再运行：\n",
    "\n",
    "  ```bash\n",
    "  uv run pytest tests/test_train_bpe.py\n",
    "  ```\n",
    "* 通过所有测试。\n",
    "\n",
    "可选进阶：\n",
    "\n",
    "* 将关键部分用 **C++（cppyy）** 或 **Rust（PyO3）** 实现，加速性能。\n",
    "* 注意 Python 与底层语言间的 **内存拷贝 vs 直接读写**。\n",
    "* 确保能通过 `pyproject.toml` 编译。\n",
    "* 注意 GPT-2 使用的正则较复杂，很多引擎不支持。\n",
    "  推荐使用 **Oniguruma** 或 Python 的 `regex` 包（速度更快）。\n",
    "\n",
    "---\n",
    "\n",
    "### 题目 (train\\_bpe\\_tinystories)：TinyStories 上的 BPE 训练（2 分）\n",
    "\n",
    "(a) 在 TinyStories 数据集上训练 **字节级 BPE 分词器**：\n",
    "\n",
    "* 最大词表大小：**10,000**\n",
    "* 添加特殊标记：`<|endoftext|>`\n",
    "\n",
    "要求：\n",
    "\n",
    "* 将生成的 **词表** 和 **合并序列** 保存到磁盘，方便检查。\n",
    "* 回答：\n",
    "\n",
    "  * 训练花了多少时间和内存？\n",
    "  * 词表中最长的 token 是什么？它合理吗？\n",
    "\n",
    "资源要求：\n",
    "\n",
    "* ≤ 30 分钟（CPU）\n",
    "* ≤ 30GB 内存\n",
    "\n",
    "提示：\n",
    "\n",
    "* 使用多进程加速预分词，可在 **2 分钟以内**完成训练。\n",
    "* TinyStories 的文档边界用 `<|endoftext|>`。\n",
    "* `<|endoftext|>` 在 BPE 合并前需要特殊处理。\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "(b) **代码性能分析（profiling）**\n",
    "\n",
    "* 哪个部分最耗时？\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "### 题目 (train\\_bpe\\_expts\\_owt)：OpenWebText 上的 BPE 训练（2 分）\n",
    "\n",
    "(a) 在 OpenWebText 数据集上训练：\n",
    "\n",
    "* 最大词表大小：**32,000**\n",
    "* 保存生成的词表和合并序列。\n",
    "\n",
    "回答：\n",
    "\n",
    "* 词表中最长的 token 是什么？合理吗？\n",
    "\n",
    "资源要求：\n",
    "\n",
    "* ≤ 12 小时（CPU）\n",
    "* ≤ 100GB 内存\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "(b) 对比：\n",
    "\n",
    "* 在 TinyStories 上训练得到的分词器\n",
    "* 在 OpenWebText 上训练得到的分词器\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "要不要我帮你写一个 **BPE 训练函数（train\\_bpe）** 的 Python 框架代码（带并行预分词和缓存优化的结构），让你能直接往里补充逻辑并通过测试？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d0c3e",
   "metadata": {},
   "source": [
    "# 2.6 BPE分词器：编码与解码\n",
    "在本作业的前一部分，我们实现了一个函数，可在输入文本上训练BPE（字节对编码）分词器，以获得分词器词汇表和BPE合并操作列表。现在，我们将实现一个BPE分词器，该分词器能加载给定的词汇表和合并操作列表，并使用它们将文本与token ID（令牌ID）进行双向转换（文本编码为token ID、token ID解码为文本）。\n",
    "\n",
    "\n",
    "## 2.6.1 文本编码\n",
    "BPE对文本进行编码的过程，与我们训练BPE词汇表的过程相似，主要包含以下几个步骤：\n",
    "\n",
    "### 步骤1：预分词（Pre-tokenize）\n",
    "首先对序列进行预分词处理，然后将每个预分词表示为UTF-8字节序列——这与我们在BPE训练中所做的操作一致。我们会在每个预分词内部将这些字节合并为词汇表中的元素，且**不会跨预分词边界进行合并**（即每个预分词的处理是独立的）。\n",
    "\n",
    "### 步骤2：应用合并操作\n",
    "随后，我们获取在BPE训练过程中生成的“词汇表元素合并操作列表”，并按照该列表的**生成顺序**，将这些合并操作应用到我们的预分词上。\n",
    "\n",
    "\n",
    "#### 示例（bpe_encoding）：BPE编码示例\n",
    "假设输入字符串为`'the cat ate'`，词汇表为`{0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'th', 7: b' c', 8: b' a', 9: b'the', 10: b'at'}`，已学习到的合并操作列表为`[(b't', b'h'), (b' ', b'c'), (b' ', 'a'), (b'th', b'e'), (b' a', b't')]`。具体编码过程如下：\n",
    "1. 首先，预分词器会将输入字符串拆分为`['the', ' cat', ' ate']`（注意空格与后续字符的绑定，如`' cat'`表示空格+“cat”，`' ate'`表示空格+“ate”）。\n",
    "2. 接着，我们逐个处理每个预分词，并应用BPE合并操作：\n",
    "   - 第一个预分词`'the'`最初表示为字节序列`[b't', b'h', b'e']`。查看合并操作列表，找到第一个可应用的合并操作`(b't', b'h')`，将该预分词转换为`[b'th', b'e']`；之后再次回溯合并操作列表，找到下一个可应用的合并操作`(b'th', b'e')`，进一步将预分词转换为`[b'the']`。此时，该预分词已完全合并为单个token，无更多可应用的合并操作，处理结束。其对应的整数序列（token ID）为`[9]`。\n",
    "   - 对第二个预分词`' cat'`重复上述过程：应用合并操作后，最终表示为字节序列`[b' c', b'a', b't']`，对应的整数序列为`[7, 1, 5]`。\n",
    "   - 对第三个预分词`' ate'`重复上述过程：应用合并操作后，最终表示为字节序列`[b' at', b'e']`，对应的整数序列为`[10, 3]`。\n",
    "\n",
    "最终，输入字符串`'the cat ate'`编码后的结果为`[9, 7, 1, 5, 10, 3]`。\n",
    "\n",
    "\n",
    "### 特殊token（Special tokens）\n",
    "你的分词器在编码文本时，应能正确处理用户定义的特殊token（在构建分词器时提供）。\n",
    "\n",
    "\n",
    "### 内存考量\n",
    "假设我们需要对一个无法完全加载到内存中的大型文本文件进行分词。为了高效地对该大型文件（或任何其他数据流）进行分词，我们需要将其拆分为可管理的块（chunk），并逐个处理每个块——这样内存复杂度会保持恒定，而非与文本大小呈线性关系。在此过程中，必须确保**token不会跨越块边界**，否则得到的分词结果将与“将整个序列加载到内存中进行分词”的常规方法不一致。\n",
    "\n",
    "\n",
    "## 2.6.2 文本解码\n",
    "要将整数token ID序列解码回原始文本，只需执行以下步骤：\n",
    "1. 查找每个token ID在词汇表中对应的条目（即字节序列）；\n",
    "2. 将这些字节序列连接起来；\n",
    "3. 将连接后的字节解码为Unicode字符串。\n",
    "\n",
    "需要注意的是，输入的token ID序列**不保证一定能映射到有效的Unicode字符串**（因为用户可能输入任意整数序列）。若输入的token ID无法生成有效的Unicode字符串，应使用标准Unicode替换字符`U+FFFD`（即“�”）替换畸形字节。在Python中，`bytes.decode()`方法的`errors`参数可控制Unicode解码错误的处理方式，使用`errors='replace'`会自动将畸形数据替换为上述替换字符。\n",
    "\n",
    "\n",
    "## 任务（tokenizer）：实现分词器（15分）\n",
    "### 交付要求\n",
    "实现一个`Tokenizer`类，该类接收词汇表（vocab）和合并操作列表（merges），可将文本编码为整数ID，也可将整数ID解码为文本。该分词器还需支持用户提供的特殊token（若特殊token未在词汇表中，则将其追加到词汇表中）。建议采用以下接口设计：\n",
    "\n",
    "\n",
    "#### 1. `__init__(self, vocab, merges, special_tokens=None)`\n",
    "基于给定的词汇表、合并操作列表和（可选的）特殊token列表，构建一个分词器。该函数需接收以下参数：\n",
    "- `vocab`: dict[int, bytes] —— 词汇表，键为token ID（整数），值为对应的字节序列\n",
    "- `merges`: list[tuple[bytes, bytes]] —— BPE合并操作列表，每个元素为两个字节序列组成的元组（表示需合并的两个元素）\n",
    "- `special_tokens`: list[str] | None = None —— 特殊token列表（可选），元素为字符串类型\n",
    "\n",
    "\n",
    "#### 2. `from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)`\n",
    "类方法，基于序列化的词汇表文件、合并操作列表文件（格式与你在BPE训练代码中输出的格式一致）和（可选的）特殊token列表，构建并返回一个`Tokenizer`实例。该方法需额外接收以下参数：\n",
    "- `vocab_filepath`: str —— 词汇表文件的路径\n",
    "- `merges_filepath`: str —— 合并操作列表文件的路径\n",
    "- `special_tokens`: list[str] | None = None —— 特殊token列表（可选）\n",
    "\n",
    "\n",
    "#### 3. `encode(self, text: str) -> list[int]`\n",
    "将输入文本编码为token ID序列，返回整数列表。\n",
    "\n",
    "\n",
    "#### 4. `encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]`\n",
    "接收一个字符串可迭代对象（例如Python文件句柄），返回一个生成器（generator），用于惰性生成token ID（即按需逐个输出，而非一次性生成所有ID）。该方法是为了实现对大型文件的内存高效分词（无需将整个文件加载到内存）。\n",
    "\n",
    "\n",
    "#### 5. `decode(self, ids: list[int]) -> str`\n",
    "将token ID序列解码为文本，返回字符串。\n",
    "\n",
    "\n",
    "### 测试说明\n",
    "要使用我们提供的测试用例验证你的`Tokenizer`实现，需先在`[adapters.get_tokenizer]`处实现测试适配器（test adapter），然后运行命令`uv run pytest tests/test_tokenizer.py`。你的实现需能通过所有测试用例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087b1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
