{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6267b8f8",
   "metadata": {},
   "source": [
    "好的，我来把这段作业说明完整翻译成中文，保持原有的层次和要求。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 训练 BPE 分词器实验\n",
    "\n",
    "我们将要在 **TinyStories 数据集** 上训练一个 **字节级 BPE（Byte Pair Encoding）分词器**。\n",
    "（关于如何查找 / 下载该数据集的说明，请见 **第 1 节**。）\n",
    "在开始前，我们建议先浏览一下 TinyStories 数据集，以便对数据内容有一个直观的认识。\n",
    "\n",
    "---\n",
    "\n",
    "#### 并行化预分词（Pre-tokenization）\n",
    "\n",
    "你会发现，最大的瓶颈在于 **预分词步骤**。\n",
    "你可以通过 **多进程库 multiprocessing** 来并行化预分词，从而加速。\n",
    "\n",
    "具体来说，建议的做法是：\n",
    "\n",
    "* 将语料划分为多个 **chunk（块）**。\n",
    "* 确保 **chunk 边界出现在特殊标记的开头**。\n",
    "\n",
    "👉 我们推荐直接使用下面提供的 starter code 来获取 chunk 边界：\n",
    "[https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336\\_basics/pretokenization\\_example.py](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py)\n",
    "\n",
    "这样切分总是合法的，因为我们永远不希望跨文档边界进行 merge 操作。\n",
    "在作业中，你可以始终采用这种方式，不必担心「超大语料中没有 `<|endoftext|>`」的边缘情况。\n",
    "\n",
    "---\n",
    "\n",
    "#### 在预分词前移除特殊标记\n",
    "\n",
    "在运行正则表达式（`re.finditer`）进行预分词之前，\n",
    "你应该先从语料（或每个 chunk）中去除所有 **特殊标记**。\n",
    "\n",
    "⚠️ 关键点：要对语料进行 **基于特殊标记的切分**，保证 **不会在特殊标记两侧合并**。\n",
    "\n",
    "例如：\n",
    "\n",
    "```\n",
    "[Doc 1]<|endoftext|>[Doc 2]\n",
    "```\n",
    "\n",
    "应当被切分为：\n",
    "\n",
    "* 对 `[Doc 1]` 单独预分词\n",
    "* 对 `[Doc 2]` 单独预分词\n",
    "\n",
    "这样就不会跨文档边界合并。\n",
    "\n",
    "实现方法：\n",
    "可以使用 `re.split`，以 `\"|\".join(special_tokens)` 作为分隔符（注意要配合 `re.escape`，因为特殊标记中可能包含 `|`）。\n",
    "\n",
    "👉 测试用例 **test\\_train\\_bpe\\_special\\_tokens** 会检查这一点。\n",
    "\n",
    "---\n",
    "\n",
    "#### 优化合并步骤（BPE Training）\n",
    "\n",
    "朴素的 BPE 训练实现非常慢，因为：\n",
    "\n",
    "* 每次合并都要重新扫描所有字节对，找到出现频率最高的 pair。\n",
    "\n",
    "改进思路：\n",
    "\n",
    "* 实际上，每次合并后，只有 **与被合并 pair 相邻的 pair 频率** 会发生变化。\n",
    "* 因此，可以通过 **维护 pair 频率的索引（缓存）并增量更新** 来加速，而不是每次都全量扫描。\n",
    "\n",
    "⚠️ 注意：BPE 训练的 **合并部分无法在 Python 中并行化**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 低资源 / 下采样（Downscaling）提示\n",
    "\n",
    "* 使用 **cProfile** 或 **scalene** 等分析工具，定位瓶颈代码，并集中优化。\n",
    "* 在正式跑大规模训练前，**先用小数据子集调试**。\n",
    "\n",
    "  * 例如，可以先在 TinyStories **验证集（22K 文档）** 上训练，而不是完整的 **2.12M 文档**。\n",
    "\n",
    "这样做的意义：\n",
    "\n",
    "* 调试时更快。\n",
    "* 保证小数据集仍然能复现大数据集的性能瓶颈，从而优化能迁移到正式环境。\n",
    "\n",
    "---\n",
    "\n",
    "### 题目 (train\\_bpe)：BPE 分词器训练（15 分）\n",
    "\n",
    "**任务**：编写一个函数，给定输入文本路径，训练一个 **字节级 BPE 分词器**。\n",
    "\n",
    "函数需支持以下参数：\n",
    "\n",
    "* `input_path: str` —— 训练数据文件路径。\n",
    "* `vocab_size: int` —— 最大词表大小（包括初始字节词表、合并得到的新词，以及所有特殊标记）。\n",
    "* `special_tokens: list[str]` —— 需要添加到词表中的特殊标记。⚠️ 这些特殊标记**不影响 BPE 训练过程**。\n",
    "\n",
    "函数返回：\n",
    "\n",
    "* `vocab: dict[int, bytes]` —— 分词器词表（token ID → 对应字节序列）。\n",
    "* `merges: list[tuple[bytes, bytes]]` —— 训练得到的 BPE 合并操作序列，每一项是一个合并的 token 对，顺序为生成顺序。\n",
    "\n",
    "测试方法：\n",
    "\n",
    "* 先实现 **\\[adapters.run\\_train\\_bpe]** 测试适配器。\n",
    "* 再运行：\n",
    "\n",
    "  ```bash\n",
    "  uv run pytest tests/test_train_bpe.py\n",
    "  ```\n",
    "* 通过所有测试。\n",
    "\n",
    "可选进阶：\n",
    "\n",
    "* 将关键部分用 **C++（cppyy）** 或 **Rust（PyO3）** 实现，加速性能。\n",
    "* 注意 Python 与底层语言间的 **内存拷贝 vs 直接读写**。\n",
    "* 确保能通过 `pyproject.toml` 编译。\n",
    "* 注意 GPT-2 使用的正则较复杂，很多引擎不支持。\n",
    "  推荐使用 **Oniguruma** 或 Python 的 `regex` 包（速度更快）。\n",
    "\n",
    "---\n",
    "\n",
    "### 题目 (train\\_bpe\\_tinystories)：TinyStories 上的 BPE 训练（2 分）\n",
    "\n",
    "(a) 在 TinyStories 数据集上训练 **字节级 BPE 分词器**：\n",
    "\n",
    "* 最大词表大小：**10,000**\n",
    "* 添加特殊标记：`<|endoftext|>`\n",
    "\n",
    "要求：\n",
    "\n",
    "* 将生成的 **词表** 和 **合并序列** 保存到磁盘，方便检查。\n",
    "* 回答：\n",
    "\n",
    "  * 训练花了多少时间和内存？\n",
    "  * 词表中最长的 token 是什么？它合理吗？\n",
    "\n",
    "资源要求：\n",
    "\n",
    "* ≤ 30 分钟（CPU）\n",
    "* ≤ 30GB 内存\n",
    "\n",
    "提示：\n",
    "\n",
    "* 使用多进程加速预分词，可在 **2 分钟以内**完成训练。\n",
    "* TinyStories 的文档边界用 `<|endoftext|>`。\n",
    "* `<|endoftext|>` 在 BPE 合并前需要特殊处理。\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "(b) **代码性能分析（profiling）**\n",
    "\n",
    "* 哪个部分最耗时？\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "### 题目 (train\\_bpe\\_expts\\_owt)：OpenWebText 上的 BPE 训练（2 分）\n",
    "\n",
    "(a) 在 OpenWebText 数据集上训练：\n",
    "\n",
    "* 最大词表大小：**32,000**\n",
    "* 保存生成的词表和合并序列。\n",
    "\n",
    "回答：\n",
    "\n",
    "* 词表中最长的 token 是什么？合理吗？\n",
    "\n",
    "资源要求：\n",
    "\n",
    "* ≤ 12 小时（CPU）\n",
    "* ≤ 100GB 内存\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "(b) 对比：\n",
    "\n",
    "* 在 TinyStories 上训练得到的分词器\n",
    "* 在 OpenWebText 上训练得到的分词器\n",
    "\n",
    "**交付物**：一句话或两句话回答。\n",
    "\n",
    "---\n",
    "\n",
    "要不要我帮你写一个 **BPE 训练函数（train\\_bpe）** 的 Python 框架代码（带并行预分词和缓存优化的结构），让你能直接往里补充逻辑并通过测试？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087b1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
