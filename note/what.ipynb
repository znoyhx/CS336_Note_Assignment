{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1494b1a6",
   "metadata": {},
   "source": [
    "daf1\n",
    "**fasd**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa337008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0652d",
   "metadata": {},
   "source": [
    "Python会自动显示将bytes的字节转化为ASCII对应的字符\n",
    "1. bytes([i]) 的本质\n",
    "\n",
    "它不会主动做 ASCII 转换，只是把整数 i（范围 0–255） 变成一个字节。\n",
    "\n",
    "字节本质就是一个 0–255 的数。\n",
    "\n",
    "比如 bytes([65]) 就是一个只包含 0x41 的字节序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124029e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d81b484",
   "metadata": {},
   "source": [
    "Unicode 是“字符的编号表”（码点表），chr()/ord()是 Python 里的操作方式，U+XXXX是官方表示法。\n",
    "\n",
    "编码（UTF-8/UTF-16等）是把这些编号 变成实际存储的字节，这是电脑能理解的形式\n",
    "\n",
    "text = \"Hi! 你好\"\n",
    "b = text.encode(\"utf-8\")\n",
    "print(b)  # b'Hi! \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd'\n",
    "| 字符  | UTF-8 字节           |\n",
    "| --- | ------------------ |\n",
    "| 'H' | 1 (0x48)           |\n",
    "| 'i' | 1 (0x69)           |\n",
    "| '!' | 1 (0x21)           |\n",
    "| ' ' | 1 (0x20)           |\n",
    "| '你' | 3 (0xE4 0xBD 0xA0) |\n",
    "| '好' | 3 (0xE5 0xA5 0xBD) |\n",
    "总字节数 = 1+1+1+1+3+3 = 10\n",
    "\n",
    "所以 b 是一个 长度为 10 的 bytes 对象："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cc56a",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings.\n",
    "UTF-8 is single bytes, and other are 2 or 4 bytes to represent one 字, like 我 ，in UTF-8, it like /xe2/x34（瞎写的） ,可以按字节解析 consist of two bytes, but in UTF-16, it would like /xe234 , 太长过于复杂\n",
    "Deliverable: A one-to-two sentence response.\n",
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results.\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "因为不能逐字节解析， 比如‘我’是/xe2/x34 ，若逐字节解析的话， 看到/xe2便不知道解析什么了\n",
    "\n",
    "###Unicode和UTF-8的编码关系\n",
    "\n",
    "Unicode 码点是“字符的编号”，比如 '我' = U+6211 = 25105\n",
    "\n",
    "UTF-8是一种把这个编号存成字节的规则，让计算机能存储或传输\n",
    "\n",
    "规则就是告诉你：\n",
    "\n",
    "小于 128 的字符 → 1 个字节\n",
    "\n",
    "128~2047 → 2 个字节\n",
    "\n",
    "2048~65535 → 3 个字节\n",
    "\n",
    "更大的 → 4 个字节\n",
    "\n",
    "所以 '我' 在 2048~65535 范围 → 用 3 个字节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fc0e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51641ea8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5406661e",
   "metadata": {},
   "source": [
    "当然可以！我来帮你系统地讲解 **BPE（Byte Pair Encoding）** 的必要性、分词过程，并举个例子。我们一步步来，确保你理解为什么 NLP 模型尤其是 Transformer/LLM 会用它。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、BPE 的必要性\n",
    "\n",
    "在自然语言处理（NLP）中，我们希望把文本转化为模型能处理的“符号序列”（tokens）。常见方法有：\n",
    "\n",
    "1. **字符级（Character-level）**\n",
    "\n",
    "   * 每个字符都是一个 token，例如 `\"hello\"` → `['h','e','l','l','o']`\n",
    "   * 优点：无需词典，能处理任何词\n",
    "   * 缺点：序列过长，模型学习语义变慢\n",
    "\n",
    "2. **单词级（Word-level）**\n",
    "\n",
    "   * 每个单词是一个 token，例如 `\"hello world\"` → `['hello','world']`\n",
    "   * 优点：序列短，直接有语义\n",
    "   * 缺点：\n",
    "\n",
    "     * 词表会非常大\n",
    "     * 新词或拼写错误的词无法表示（Out-of-Vocabulary，OOV 问题）\n",
    "\n",
    "3. **子词级（Subword-level）** → **BPE 就属于这一类**\n",
    "\n",
    "   * 将单词拆成子词或者字符片段\n",
    "   * 优点：\n",
    "\n",
    "     * 词表大小适中\n",
    "     * 可以处理未见过的新词\n",
    "     * 模型学习效率高\n",
    "\n",
    "> **总结**：BPE 的必要性就是在 **“序列长度可控”** 和 **“覆盖新词能力”** 之间找到平衡。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、BPE 的分词过程\n",
    "\n",
    "BPE 是一种 **基于频率的贪心算法**，核心思路是 **迭代合并最频繁的字符/子词对**。\n",
    "\n",
    "### 步骤：\n",
    "\n",
    "1. **初始化词表**\n",
    "\n",
    "   * 将所有词拆成字符 + `</w>` 表示单词结束\n",
    "   * 例如 `\"low\"` → `['l','o','w','</w>']`\n",
    "\n",
    "2. **统计所有连续字符对的频率**\n",
    "\n",
    "   * 例如有单词 `low` 和 `lowest`\n",
    "\n",
    "     ```\n",
    "     low → l o w </w>\n",
    "     lowest → l o w e s t </w>\n",
    "     频率统计：\n",
    "     ('l','o'):2, ('o','w'):2, ('w','</w>'):1, ('w','e'):1 ...\n",
    "     ```\n",
    "\n",
    "3. **合并最频繁的一对**\n",
    "\n",
    "   * 假设 ('l','o') 最频繁 → 合并成 `'lo'`\n",
    "   * 更新词表：\n",
    "\n",
    "     ```\n",
    "     low → lo w </w>\n",
    "     lowest → lo w e s t </w>\n",
    "     ```\n",
    "\n",
    "4. **重复统计和合并**，直到达到预设的词表大小或没有高频对\n",
    "\n",
    "5. **生成最终子词词表**\n",
    "\n",
    "   * 可以得到类似 `['l','o','w','lo','low','</w>','e','s','t']`\n",
    "   * 新词 `\"lower\"` 可以拆成 `['low','er']`，即使 `\"lower\"` 未在训练集中出现，也可以表示\n",
    "\n",
    "---\n",
    "\n",
    "## 三、举例说明\n",
    "\n",
    "假设我们有两个单词作为训练集：\n",
    "\n",
    "```\n",
    "low\n",
    "lower\n",
    "lowest\n",
    "```\n",
    "\n",
    "**Step 1：初始化**\n",
    "\n",
    "```\n",
    "low → l o w </w>\n",
    "lower → l o w e r </w>\n",
    "lowest → l o w e s t </w>\n",
    "```\n",
    "\n",
    "**Step 2：统计频率**\n",
    "\n",
    "```\n",
    "('l','o'):3, ('o','w'):3, ('w','</w>'):1, ('w','e'):2 ...\n",
    "```\n",
    "\n",
    "**Step 3：合并最频繁的对**\n",
    "\n",
    "1. ('l','o') → 'lo'\n",
    "\n",
    "```\n",
    "lo w </w>\n",
    "lo w e r </w>\n",
    "lo w e s t </w>\n",
    "```\n",
    "\n",
    "2. ('lo','w') → 'low'\n",
    "\n",
    "```\n",
    "low </w>\n",
    "low e r </w>\n",
    "low e s t </w>\n",
    "```\n",
    "\n",
    "3. ('e','r') → 'er'\n",
    "\n",
    "```\n",
    "low </w>\n",
    "low er </w>\n",
    "low e s t </w>\n",
    "```\n",
    "\n",
    "4. ('e','s') → 'es'\n",
    "5. ('es','t') → 'est'\n",
    "\n",
    "**Step 4：生成词表**\n",
    "\n",
    "```\n",
    "['l','o','w','low','e','r','er','es','est','</w>']\n",
    "```\n",
    "\n",
    "**新词分词**：\n",
    "\n",
    "* `\"lowest\"` → `['low','est']`\n",
    "* `\"lower\"` → `['low','er']`\n",
    "* `\"lovely\"` → `['lo','ve','l','y']`（新词也能表示）\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 总结\n",
    "\n",
    "* **BPE 必要性**：解决 **OOV（新词问题）** + 控制词表大小 + 适度缩短序列长度\n",
    "* **过程**：字符 → 统计频率 → 贪心合并 → 生成子词词表\n",
    "* **优点**：新词可拆分表示、词表小、训练高效\n",
    "\n",
    "完全正确！🎯\n",
    "\n",
    "* **训练阶段**：\n",
    "  你先用大规模语料构建 BPE 词表（也就是学习哪些字符组合最频繁，生成子词集合）。\n",
    "\n",
    "* **推理/新词阶段**：\n",
    "  当模型遇到 **训练语料里没有的新单词**，它会根据已有的子词词表，把新单词拆成已知的子词组合。\n",
    "\n",
    "比如词表里有：\n",
    "\n",
    "```\n",
    "['low','er','est','</w>','l','o','w','e','s','t']\n",
    "```\n",
    "\n",
    "* 新单词 `\"lowest\"` → `['low','est']`\n",
    "* 新单词 `\"lower\"` → `['low','er']`\n",
    "* 新单词 `\"lovely\"` → `['lo','ve','l','y']`（如果 `'lo','ve','l','y'` 都在词表里，或者可以拆到最小字符）\n",
    "\n",
    "> 这样就 **避免了 OOV（Out-Of-Vocabulary）问题**，同时保持序列长度比字符级短，比单词级灵活。\n",
    "\n",
    "而Byte的BPE就是考虑每个字节，尽管中文的UTF-8的编码一般是二到三个字节，但在训练中也会将这种组合加进去\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2aca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab3e2f65",
   "metadata": {},
   "source": [
    "\n",
    "好的，我来帮你把我们这几轮关于 **BPE（Byte Pair Encoding）在英语文本、字节级别、pre-token 限制下的工作机制** 的讨论总结一下，并给出一个贴切的例子，直观说明整个理论流程。\n",
    "\n",
    "---\n",
    "\n",
    "## **一、核心理论总结**\n",
    "\n",
    "### **1️⃣ 预分词（Pre-tokenization）**\n",
    "\n",
    "* 目的：在训练 BPE 时提供边界，避免跨单词或跨标点的冗余合并，比如dog。和dog！都会因为频率过高而统计进去，但事实上我们并不需要这种莫名其妙的组合，只需要dog\n",
    "* 方法：\n",
    "\n",
    "  * 用正则或空格切分文本\n",
    "    *正则表达式：PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"基本可以全部分出来\n",
    "    *空格：简单的划分\n",
    "  * 每个 pre-token 作为**独立的字节池**进行统计和合并，也就是所有的pretoken的组合中的各个组合的出现次数做一个整体的统计在进行操作\n",
    "* 结果：\n",
    "\n",
    "  * `\"dog!\"` 被拆成 `'dog'` 和 `'!'` 两个 pre-token\n",
    "  * 统计字节对时，绝不会统计 `'g' + '!'` 这种跨边界对，因为！只有一个字节所以不会参与合并\n",
    "\n",
    "---\n",
    "\n",
    "### **2️⃣ BPE 的基本单位：字节**\n",
    "\n",
    "* 在 byte-level BPE 中，**每个字节是最小单位**\n",
    "* 合并过程：\n",
    "\n",
    "  1. 统计每个 pre-token 内相邻字节对出现频率\n",
    "  2. 找到最频繁的 pair 进行合并\n",
    "  3. 新生成的 token 加入词表\n",
    "  4. 迭代多轮，生成更大 token（小词组 → 完整单词）\n",
    "\n",
    "---\n",
    "\n",
    "### **3️⃣ 列表表示与元素属性**\n",
    "\n",
    "* 每个 pre-token 内用列表存储 token 元素\n",
    "* 元素类型可以不同：\n",
    "\n",
    "  * 单字节整数 `[100]`\n",
    "  * 已合并的 token `'og'` 或元组 `(111,103)`\n",
    "* **相邻元素组合成 pair**参与下一轮统计\n",
    "* Python 内建 tuple 比较可以直接判断相同 pair，无需关心元素类型\n",
    "\n",
    "---\n",
    "\n",
    "### **4️⃣ BPE 合并规则**\n",
    "\n",
    "* 仅在 **pre-token 内部**进行合并\n",
    "* 高频 pair 优先合并\n",
    "* 若频率相同，按字典序（lexicographic order）决定\n",
    "* 每次合并：\n",
    "\n",
    "  * 用一个元素替换原来的两个元素\n",
    "  * 更新 pre-token 内列表\n",
    "* 迭代执行直到达到设定的 merge 次数或满足词表大小\n",
    "\n",
    "---\n",
    "\n",
    "### **5️⃣ BPE “智能”本质**\n",
    "\n",
    "* BPE 并不理解语义，只是 **频率贪心算法**\n",
    "* “看起来聪明”是因为：\n",
    "\n",
    "  * 高频字节组合 → 生成常用 token\n",
    "  * 避免标点、单词冗余 → 提高模型训练效率\n",
    "\n",
    "---\n",
    "\n",
    "## **二、贴切的例子**\n",
    "\n",
    "假设文本：\n",
    "\n",
    "```text\n",
    "\"I love dog!\"\n",
    "```\n",
    "\n",
    "### **Step 1：预分词**\n",
    "\n",
    "```python\n",
    "pre_tokens = ['I', ' love', ' dog', '!']\n",
    "```\n",
    "\n",
    "### **Step 2：UTF-8 编码**\n",
    "\n",
    "```python\n",
    "[73], [32,108,111,118,101], [32,100,111,103], [33]\n",
    "```\n",
    "\n",
    "### **Step 3：第一次 BPE 合并（字节对 'o' + 'g'）**\n",
    "\n",
    "* 匹配 `' dog'` 内 `[100,111,103]` → `'o' + 'g' → 'og'`\n",
    "* 合并后列表：\n",
    "\n",
    "```python\n",
    "[32, 100, 'og']  # 或 [32, 100, (111,103)]\n",
    "```\n",
    "\n",
    "### **Step 4：第二次 BPE 合并（'d' + 'og'）**\n",
    "\n",
    "* 匹配 `' dog'` 内 `[100, 'og'] → 'dog'`\n",
    "* 合并后列表：\n",
    "\n",
    "```python\n",
    "[32, 'dog']\n",
    "```\n",
    "\n",
    "### **Step 5：最终 token 序列**\n",
    "\n",
    "```python\n",
    "['I', ' love', ' ', 'dog', '!']  # 每个 token 可直接加入词表\n",
    "```\n",
    "\n",
    "> 注意：\n",
    ">\n",
    "> * `'dog!'` 没有被合并成 `'dog!'` token\n",
    "> * 保持了 pre-token 边界\n",
    "> * 字节 → 小 token → 完整 token 分层清晰\n",
    "\n",
    "---\n",
    "\n",
    "### **三、分层理解**\n",
    "\n",
    "| 层级  | 单位                | 合并规则            | 示例                   |\n",
    "| --- | ----------------- | --------------- | -------------------- |\n",
    "| 第一层 | 字节                | 最频繁相邻字节合并       | `'o'+'g' → 'og'`     |\n",
    "| 第二层 | pre-token 内 token | 继续合并高频组合        | `'d' + 'og' → 'dog'` |\n",
    "| 第三层 | pre-token 边界      | 不跨 pre-token 合并 | `'dog'` 与 `'!'` 不合并  |\n",
    "\n",
    "---\n",
    "\n",
    "这个例子完整演示了：\n",
    "\n",
    "1. **字节级 BPE 从最小单位合并到完整单词 token**\n",
    "2. **pre-token 边界保护标点和空格**\n",
    "3. **列表元素可以是不同属性的 token**\n",
    "4. **相邻元素可以形成 pair，无论类型**\n",
    "\n",
    "---\n",
    "\n",
    "注意到存在特殊token，应该开局就把这个特殊token丢进基础词表（256的那个）\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
